{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb297e45-2c09-4ff6-86d4-31a09e6b093f",
   "metadata": {},
   "source": [
    "# Circuit learning module: Pennylane with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e5022b3-ee75-4f3b-bb1a-5deb86ed077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import warnings\n",
    "import collections\n",
    "from pathlib import Path\n",
    "from pennylane import numpy as np\n",
    "import pickle\n",
    "import pennylane as qml\n",
    "from sympy import default_sort_key\n",
    "import torch\n",
    "from discopy.quantum.pennylane import to_pennylane, PennyLaneCircuit\n",
    "from inspect import signature\n",
    "from noisyopt import minimizeSPSA\n",
    "from utils import transform_into_pennylane_circuits, read_diagrams, get_symbols, create_labeled_classes, acc_from_dict, loss_from_dict\n",
    "\n",
    "this_folder = os.path.abspath(os.getcwd())\n",
    "nshot = 10000\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# OpenMP: number of parallel threads.\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "103fd94b-ee5c-422b-a673-57154bfcf457",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4                # Number of qubits\n",
    "step = 0.0004               # Learning rate\n",
    "batch_size = 4              # Number of samples for each training step\n",
    "num_epochs = 3              # Number of training epochs\n",
    "q_depth = 6                 # Depth of the quantum circuit (number of variational layers)\n",
    "gamma_lr_scheduler = 0.1    # Learning rate reduction applied every 10 epochs.\n",
    "q_delta = 0.01              # Initial spread of random quantum weights\n",
    "start_time = time.time()    # Start of the computation timer\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97924910-8724-432b-a8b3-e87f7d127984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select workload\n",
    "#workload = \"execution_time\"\n",
    "workload = \"cardinality\"\n",
    "\n",
    "# Select workload size\n",
    "#workload_size = \"small\"\n",
    "#workload_size = \"medium\"\n",
    "#workload_size = \"large\"\n",
    "workload_size = \"main\"\n",
    "\n",
    "classification = 2\n",
    "layers = 1\n",
    "single_qubit_params = 3\n",
    "n_wire_count = 1\n",
    "\n",
    "# Access the selected circuits\n",
    "path_name = this_folder + \"//simplified-JOB-diagrams//\"\\\n",
    "            + workload + \"//\" + workload_size + \"//circuits//\"\\\n",
    "            + str(classification) + \"//\" + str(layers) + \"_layer//\"\\\n",
    "           + str(single_qubit_params) + \"_single_qubit_params//\" + str(n_wire_count)\\\n",
    "            + \"_n_wire_count//\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2dd464e-ca76-43d3-b24d-84a4507e1178",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_circuits_paths = glob.glob(path_name + \"training//[0-9]*.p\")\n",
    "validation_circuits_paths = glob.glob(path_name + \"validation//[0-9]*.p\")\n",
    "test_circuits_paths = glob.glob(path_name + \"test//[0-9]*.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "278cac63-8ee9-42cc-ac47-6c2e2fae37df",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_circuits = read_diagrams(training_circuits_paths)\n",
    "validation_circuits = read_diagrams(validation_circuits_paths)\n",
    "test_circuits = read_diagrams(test_circuits_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ca92833-9359-43e0-8157-e60d1ac361e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data, validation_data = None, None, None\n",
    "data_path = this_folder + \"//data//\" + workload + \"//\" + workload_size + \"//\"\n",
    "\n",
    "with open(data_path + \"training_data.json\", \"r\") as inputfile:\n",
    "    training_data = json.load(inputfile)['training_data']\n",
    "with open(data_path + \"test_data.json\", \"r\") as inputfile:\n",
    "    test_data = json.load(inputfile)['test_data']\n",
    "with open(data_path + \"validation_data.json\", \"r\") as inputfile:\n",
    "    validation_data = json.load(inputfile)['validation_data']\n",
    "\n",
    "training_data_labels = create_labeled_classes(training_data, classification, workload)\n",
    "test_data_labels = create_labeled_classes(test_data, classification, workload)\n",
    "validation_data_labels = create_labeled_classes(validation_data, classification, workload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bb34de9-4780-4b21-b423-7ca8d883dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix = []\n",
    "for i in training_circuits:\n",
    "    if i not in training_data_labels:\n",
    "        fix.append(i)\n",
    "        \n",
    "for i in fix:\n",
    "    training_circuits.pop(i, None)\n",
    "\n",
    "fix = []\n",
    "for i in validation_circuits:\n",
    "    if i not in validation_data_labels:    \n",
    "        fix.append(i)\n",
    "\n",
    "for i in fix:\n",
    "    validation_circuits.pop(i, None)\n",
    "\n",
    "fix = []\n",
    "for i in test_circuits:\n",
    "    if i not in test_data_labels:\n",
    "        fix.append(i)\n",
    "        \n",
    "for i in fix:\n",
    "    test_circuits.pop(i, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bb0747e-8861-445e-bab8-fcc73025afcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training circuits:  440\n",
      "Number of validation circuits:  108\n",
      "Number of test circuits:  109\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training circuits: \", len(training_circuits))\n",
    "print(\"Number of validation circuits: \", len(validation_circuits))\n",
    "print(\"Number of test circuits: \", len(test_circuits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "902eda4f-020f-43ec-80f7-c093e5f92b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_symbols = set([elem for c in training_circuits.values() for elem in c.free_symbols])\n",
    "all_symbols = list(sorted(all_symbols, key=default_sort_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4b8b01d-5a31-4b04-8bc1-29903ec3ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_params(all_params, all_symbols, current_symbols):\n",
    "    res = []\n",
    "    for param, symbol in zip(all_params, all_symbols):\n",
    "        if symbol in current_symbols:\n",
    "            res.append(param)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11795ccc-4478-43ca-9bd8-f83992122bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_cost(all_params, circuits, labels):\n",
    "    predictions = {}\n",
    "    for c in circuits:\n",
    "        disco_circuit = circuits[c]\n",
    "        qml_circuit = to_pennylane(disco_circuit, probabilities = True)\n",
    "        current_symbols = disco_circuit.free_symbols\n",
    "        current_symbols = list(sorted(current_symbols, key=default_sort_key))\n",
    "        params = pick_params(all_params, all_symbols, current_symbols)\n",
    "        result = qml_circuit.eval(symbols=current_symbols, weights=params)\n",
    "        print(result)\n",
    "        predictions[c] = result\n",
    "    loss = loss_from_dict(predictions, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7943b1c7-faeb-4e60-8033-0678338a012d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new() received an invalid combination of arguments - got (list, requires_grad=bool), but expected one of:\n * (*, torch.device device)\n      didn't match because some of the keywords were incorrect: requires_grad\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, *, torch.device device)\n * (object data, *, torch.device device)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m rng \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m init_params_spsa \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_symbols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (list, requires_grad=bool), but expected one of:\n * (*, torch.device device)\n      didn't match because some of the keywords were incorrect: requires_grad\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, *, torch.device device)\n * (object data, *, torch.device device)\n"
     ]
    }
   ],
   "source": [
    "#rng = np.random.default_rng(0)\n",
    "#init_params_spsa = torch.Tensor([[t] for t in np.array(rng.random(len(all_symbols)))], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa23ec-c7b3-427e-9750-f215c0f70caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_hybrid = optim.Adam(model_hybrid.fc.parameters(), lr=step)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_hybrid, step_size=10, gamma=gamma_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add4e99f-fa8e-4744-9555-5e4f694dca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = 10000.0  # Large arbitrary number\n",
    "    best_acc_train = 0.0\n",
    "    best_loss_train = 10000.0  # Large arbitrary number\n",
    "    print(\"Training started:\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"validation\"]:\n",
    "            #if phase == \"train\":\n",
    "                # Set model to training mode\n",
    "            #    model.train()\n",
    "            #else:\n",
    "                # Set model to evaluate mode\n",
    "            #    model.eval()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            #n_batches = dataset_sizes[phase] // batch_size\n",
    "            #it = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                since_batch = time.time()\n",
    "                batch_size_ = len(inputs)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Track/compute gradient and make an optimization step only when training\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Print iteration results\n",
    "                running_loss += loss.item() * batch_size_\n",
    "                batch_corrects = torch.sum(preds == labels.data).item()\n",
    "                running_corrects += batch_corrects\n",
    "                print(\n",
    "                    \"Phase: {} Epoch: {}/{} Iter: {}/{} Batch time: {:.4f}\".format(\n",
    "                        phase,\n",
    "                        epoch + 1,\n",
    "                        num_epochs,\n",
    "                        it + 1,\n",
    "                        n_batches + 1,\n",
    "                        time.time() - since_batch,\n",
    "                    ),\n",
    "                    end=\"\\r\",\n",
    "                    flush=True,\n",
    "                )\n",
    "                it += 1\n",
    "\n",
    "            # Print epoch results\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            print(\n",
    "                \"Phase: {} Epoch: {}/{} Loss: {:.4f} Acc: {:.4f}        \".format(\n",
    "                    \"train\" if phase == \"train\" else \"validation  \",\n",
    "                    epoch + 1,\n",
    "                    num_epochs,\n",
    "                    epoch_loss,\n",
    "                    epoch_acc,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Check if this is the best model wrt previous epochs\n",
    "            if phase == \"validation\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == \"validation\" and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "            if phase == \"train\" and epoch_acc > best_acc_train:\n",
    "                best_acc_train = epoch_acc\n",
    "            if phase == \"train\" and epoch_loss < best_loss_train:\n",
    "                best_loss_train = epoch_loss\n",
    "\n",
    "            # Update learning rate\n",
    "            if phase == \"train\":\n",
    "                scheduler.step()\n",
    "\n",
    "    # Print final results\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    time_elapsed = time.time() - since\n",
    "    print(\n",
    "        \"Training completed in {:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60)\n",
    "    )\n",
    "    print(\"Best test loss: {:.4f} | Best test accuracy: {:.4f}\".format(best_loss, best_acc))\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
